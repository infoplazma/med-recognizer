Вот что делает этот кусок:

```python
from functools import lru_cache
from sentence_transformers import SentenceTransformer

@lru_cache(maxsize=1)
def get_st_model_cached(model_name: str = "all-MiniLM-L6-v2") -> SentenceTransformer:
    return SentenceTransformer(model_name)
```

### Построчно

* `lru_cache` — встроенный декоратор, который **кэширует результаты** вызова функции по её аргументам (ключ — кортеж позиционных + именованных аргументов). Повторный вызов с теми же аргументами возвращает **уже готовый объект** из кэша, не вызывая функцию заново.

* `maxsize=1` — в кэше хранится **только одно** последнее значение.
  Это означает:

  * если вы всегда вызываете `get_st_model_cached()` **с одним и тем же `model_name`**, модель загрузится **один раз** за процесс, а дальше будет браться из кэша;
  * если начнёте вызывать с **другими именами моделей**, кэш будет держать только **последнюю использованную** (LRU-выделение).

* Тело функции создаёт `SentenceTransformer(model_name)` — **тяжёлый объект** (загрузка весов). Благодаря кэшированию этот объект будет создаваться **однократно** для каждого уникального набора аргументов (при `maxsize=1` — фактически одного актуального имени модели).

### Почему это удобно

* **Ленивая инициализация**: модель грузится при первом обращении.
* **Потокобезопасно**: `lru_cache` внутри использует замок, так что параллельные потоки не создадут несколько копий одновременно.
* **Просто**: не нужно доп. глобальных переменных/локов.

### Поведение кэша

* Кэш живёт **в рамках процесса**. В другом процессе модель снова загрузится.
* Чтобы очистить кэш вручную (например, сменить модель/устройство):

  ```python
  get_st_model_cached.cache_clear()
  ```
* Посмотреть статистику:

  ```python
  print(get_st_model_cached.cache_info())
  # CacheInfo(hits=..., misses=..., maxsize=1, currsize=1)
  ```

### Частые вопросы и подводные

* **Аргументы — часть ключа**. Если добавите, например, `device: str = "cpu"`, и будете вызывать с разными девайсами, кэш начнёт хранить разные варианты (но при `maxsize=1` будет выживать только последний):

  ```python
  @lru_cache(maxsize=1)
  def get_st_model_cached(model_name: str = "all-MiniLM-L6-v2", device: str = "cpu"):
      return SentenceTransformer(model_name, device=device)
  ```

  Если устройство может меняться в рантайме, **включите его в сигнатуру**, или фиксируйте его один раз на старте.

* **Jupyter / hot-reload**. При перезагрузке ячейки/модуля функция переопределяется, кэш теряется — это нормально.

* **Память**. Модель остаётся в памяти до завершения процесса или очистки кэша. Это обычно то, что нужно.

### Пример использования

```python
# первый вызов — загрузит модель с диска/из сети
model = get_st_model_cached()  # "all-MiniLM-L6-v2"

# дальше — мгновенно из кэша
model2 = get_st_model_cached()

# смена модели: старая запись будет вытеснена (maxsize=1)
get_st_model_cached.cache_clear()
model_mpnet = get_st_model_cached("all-mpnet-base-v2")
```

Если вам нужно **кэшировать несколько разных моделей одновременно**, увеличьте `maxsize` (например, `maxsize=3`). Если всегда одна — `1` оптимально.
